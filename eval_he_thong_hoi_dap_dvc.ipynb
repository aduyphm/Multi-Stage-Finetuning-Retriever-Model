{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8590cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, InputExample\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from torch.utils.data import DataLoader\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6018f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu test\n",
    "with open(\"dvc_test.json\", 'r') as file:\n",
    "    test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73eaaa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu các văn bản\n",
    "vbpl = pd.read_csv('sent_truncated_vbpl_update.csv', encoding='utf-16')\n",
    "vbpl = vbpl.dropna().reset_index(drop=True)\n",
    "passages = vbpl['truncated_text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6eff8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segment(sent):\n",
    "    sent = tokenize(sent.encode('utf-8').decode('utf-8'))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d42e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f2(precision, recall):        \n",
    "    return (5 * precision * recall) / (4 * precision + recall + 1e-20)\n",
    "def calculate_f1(precision, recall):        \n",
    "    return (precision * recall) / (precision + recall + 1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9b361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/simcse-model-phobert-base-1x.pkl\", \"r\") as file:\n",
    "    bi_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33aea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/ce-model-05.pkl\", \"r\") as file:\n",
    "    cross_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24051b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"model/bm25_plus\", \"rb\") as bm_file:\n",
    "    bm25 = pickle.load(bm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4b3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.read_index(\"model/case_1x.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405e9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(hits, k, scores):\n",
    "    true_positive_set = set()\n",
    "    false_positive_set = set()\n",
    "    num_hits = 0\n",
    "    average_precision = 0\n",
    "    actual_relevance = []\n",
    "      \n",
    "    for j, idx_pred in enumerate(hits[:k].index, 1):\n",
    "        key = (hits.at[idx_pred, \"so_hieu\"], hits.at[idx_pred, \"dieu\"])\n",
    "        if key in dict_relevant:\n",
    "            actual_relevance.append(1)\n",
    "            true_positive_set.add(key)\n",
    "            num_hits += 1\n",
    "            average_precision += num_hits/j\n",
    "        else:\n",
    "            actual_relevance.append(0)\n",
    "            false_positive_set.add(key)\n",
    "\n",
    "    true_positive = len(true_positive_set)            \n",
    "    false_positive = len(false_positive_set)\n",
    "    \n",
    "    if num_hits != 0: \n",
    "        average_precision = average_precision/num_hits\n",
    "    \n",
    "    ndcg = ndcg_score([actual_relevance], [scores[:k]], k=k)\n",
    "    \n",
    "    precision = true_positive/(true_positive + false_positive + 1e-20)\n",
    "    recall = true_positive/actual_positive\n",
    "    f1 = calculate_f1(precision, recall)\n",
    "    f2 = calculate_f2(precision, recall)\n",
    "    return precision, recall, f1, f2, average_precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79233cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [2:16:28, 14.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá với dữ liệu\n",
    "top_k = 500\n",
    "top_k1 = 20\n",
    "top_k2 = 5\n",
    "thresh = 0\n",
    "k1_total_f1 = 0\n",
    "k1_total_f2 = 0\n",
    "k1_total_precision = 0\n",
    "k1_total_recall = 0\n",
    "k1_total_map = 0\n",
    "k1_total_ndcg = 0\n",
    "k2_total_f1 = 0\n",
    "k2_total_f2 = 0\n",
    "k2_total_precision = 0\n",
    "k2_total_recall = 0\n",
    "k2_total_map = 0\n",
    "k2_total_ndcg = 0\n",
    "k3_total_f1 = 0\n",
    "k3_total_f2 = 0\n",
    "k3_total_precision = 0\n",
    "k3_total_recall = 0\n",
    "k3_total_map = 0\n",
    "k3_total_ndcg = 0\n",
    "\n",
    "\n",
    "for i, item in tqdm(enumerate(test)):\n",
    "    query = item[\"noi_dung_hoi\"]\n",
    "    query_bm25 = query[:-1] if query.endswith(\"?\") else query\n",
    "    relevant_articles = item[\"vb_lien_quan\"]\n",
    "    dict_relevant = {(article[\"so_hieu\"], article[\"dieu\"]) : article for article in relevant_articles}\n",
    "    actual_positive = len(relevant_articles)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Block 1: Retrieve top k using BM25+ ####\n",
    "    query_seg = word_segment(query_bm25)\n",
    "    query_tokens = query_seg.split()\n",
    "    doc_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    predictions = np.argpartition(doc_scores, len(doc_scores) - top_k)[-top_k:]\n",
    "    hits_bm25 = vbpl.iloc[predictions]\n",
    "    \n",
    "    hits3 = hits_bm25.copy()\n",
    "    hits3[\"bm25-score\"] = doc_scores[:top_k] / (np.linalg.norm(doc_scores[:top_k])+1e-20)\n",
    "    \n",
    "    #### Rerank using bi-encoder top k1 ####\n",
    "    embeddings = bi_encoder.encode(hits3['truncated_text'].to_list())\n",
    "    query_embeddings = bi_encoder.encode([query])\n",
    "    cos_scores = []\n",
    "    for embedding in embeddings:\n",
    "        cos_score = util.cos_sim(query_embeddings, embedding)\n",
    "        cos_scores.append(cos_score.numpy()[0,0])\n",
    "    hits3['cos-score'] = cos_scores\n",
    "#     hits3['ensem-score'] = (1/2*hits3['cos-score']**2 + 1/2*hits3['bm25-score']**2)**1/2\n",
    "    hits3['ensem-score'] = (hits3['cos-score'] * hits3['bm25-score'])**1/2\n",
    "    \n",
    "    hits3 = hits3.sort_values('ensem-score', ascending=False)\n",
    "    \n",
    "#     #### Rerank using cross-encoder ####  \n",
    "#     cross_inp = [[query, passages[idx]] for idx in hits3[:top_k1].index]\n",
    "#     cross_scores = cross_encoder.predict(cross_inp)\n",
    "#     hits4 = hits3[:top_k1].copy()\n",
    "#     hits4['cross-score'] = cross_scores\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Block 2: Retrieve top k using SimCSE ####\n",
    "    query_embedding = bi_encoder.encode([query])\n",
    "    normalized_query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    scores, indices, embeddings = index.search_and_reconstruct(normalized_query_embedding, top_k)\n",
    "    hits_simcse = vbpl.iloc[indices[0]]\n",
    "    hits1 = hits_simcse.copy()\n",
    "    hits1['cos-score'] = scores[0]\n",
    "    \n",
    "    #### Rerank using BM25+ 2-grams top k1 ####\n",
    "    corpus = []\n",
    "    ngram_corpus2 = []\n",
    "    for p in hits_simcse['truncated_text']:\n",
    "        p = word_segment(p)\n",
    "        p_tokens = p.split()\n",
    "        corpus.append(p_tokens)\n",
    "        ngram_corpus2 = [[ngram for ngram in zip(*[tokens[i:] for i in range(2)])] for tokens in corpus]\n",
    "    \n",
    "    bm25_rerank = BM25Plus(ngram_corpus2)\n",
    "    \n",
    "    query_seg = word_segment(query)\n",
    "    query_tokens = query_seg.split()\n",
    "    query_tokens_2 = [ngram for ngram in zip(*[query_tokens[i:] for i in range(2)])]\n",
    "    bm25_scores = bm25_rerank.get_scores(query_tokens_2)\n",
    "    hits1['bm25-score'] = bm25_scores / (np.linalg.norm(bm25_scores)+1e-20)\n",
    "#     hits1['ensem-score'] = (1/2*hits1['bm25-score']**2 + 1/2*hits1['score']**2)**1/2\n",
    "    hits1['ensem-score'] = (hits1['bm25-score']*hits1['cos-score'])**1/2\n",
    "    hits1 = hits1.sort_values('ensem-score', ascending=False)\n",
    "    \n",
    "#     #### Rerank using cross-encoder ####    \n",
    "#     cross_inp = [[query, passages[idx]] for idx in hits1[:top_k1].index]\n",
    "#     cross_scores = cross_encoder.predict(cross_inp)\n",
    "#     hits2 = hits1[:top_k1].copy()\n",
    "#     hits2['cross-score'] = cross_scores\n",
    "    \n",
    "    \n",
    "    #### Block 3: Ensemble Stage ####\n",
    "    combine = pd.concat([hits1[:20], hits3[:20]])\n",
    "    combine = combine.sort_values('ensem-score', ascending=False)\n",
    "    hits_ensem = combine.drop_duplicates(subset=['so_hieu', 'dieu', 'truncated_text'], keep='first')\n",
    "    \n",
    "#     hits_ensem = hits_ensem.sort_values('ensem-score', ascending=False)\n",
    "    \n",
    "    precision, recall, f1, f2, average_precision, ndcg = evaluate(hits=hits_ensem, k=top_k1, scores=hits_ensem['ensem-score'].to_list())\n",
    "\n",
    "    k1_total_precision += precision\n",
    "    k1_total_recall += recall\n",
    "    k1_total_f1 += f1\n",
    "    k1_total_f2 += f2\n",
    "    k1_total_map += average_precision\n",
    "    k1_total_ndcg += ndcg\n",
    "    \n",
    "    precision, recall, f1, f2, average_precision, ndcg = evaluate(hits=hits_ensem, k=top_k2, scores=hits_ensem['ensem-score'].to_list())\n",
    "\n",
    "    k2_total_precision += precision\n",
    "    k2_total_recall += recall\n",
    "    k2_total_f1 += f1\n",
    "    k2_total_f2 += f2\n",
    "    k2_total_map += average_precision\n",
    "    k2_total_ndcg += ndcg\n",
    "    \n",
    "    precision, recall, f1, f2, average_precision, ndcg = evaluate(hits=hits_ensem, k=hits_ensem.shape[0], scores=hits_ensem['ensem-score'].to_list())\n",
    "\n",
    "    k3_total_precision += precision\n",
    "    k3_total_recall += recall\n",
    "    k3_total_f1 += f1\n",
    "    k3_total_f2 += f2\n",
    "    k3_total_map += average_precision\n",
    "    k3_total_ndcg += ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305b22ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 20\n",
      "Recall: 0.8634978671541742\n",
      "Precision: 0.06480890732104808\n",
      "F2: 0.24228212818558523\n",
      "F1: 0.05956218072503938\n",
      "MAP: 0.6286834254454082\n",
      "NDCG: 0.7053359014830364\n"
     ]
    }
   ],
   "source": [
    "N = len(test)\n",
    "print(f\"k = {top_k1}\")\n",
    "print(f\"Recall: {k1_total_recall/N}\")\n",
    "print(f\"Precision: {k1_total_precision/N}\")\n",
    "print(f\"F2: {k1_total_f2/N}\")\n",
    "print(f\"F1: {k1_total_f1/N}\")\n",
    "print(f\"MAP: {k1_total_map/N}\")\n",
    "print(f\"NDCG: {k1_total_ndcg/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d0dbc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "Recall: 0.7583790371724559\n",
      "Precision: 0.224314442413163\n",
      "F2: 0.49298741072049934\n",
      "F1: 0.16721946548272038\n",
      "MAP: 0.6456073532398938\n",
      "NDCG: 0.6854849443011977\n"
     ]
    }
   ],
   "source": [
    "N = len(test)\n",
    "print(f\"k = {top_k2}\")\n",
    "print(f\"Recall: {k2_total_recall/N}\")\n",
    "print(f\"Precision: {k2_total_precision/N}\")\n",
    "print(f\"F2: {k2_total_f2/N}\")\n",
    "print(f\"F1: {k2_total_f1/N}\")\n",
    "print(f\"MAP: {k2_total_map/N}\")\n",
    "print(f\"NDCG: {k2_total_ndcg/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "202d1626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 40\n",
      "Recall: 0.8895490554539914\n",
      "Precision: 0.04747139764060302\n",
      "F2: 0.19018464250455538\n",
      "F1: 0.04459440720959348\n",
      "MAP: 0.6280865369375142\n",
      "NDCG: 0.7109856557674793\n"
     ]
    }
   ],
   "source": [
    "N = len(test)\n",
    "print(f\"k = {2*top_k1}\")\n",
    "print(f\"Recall: {k3_total_recall/N}\")\n",
    "print(f\"Precision: {k3_total_precision/N}\")\n",
    "print(f\"F2: {k3_total_f2/N}\")\n",
    "print(f\"F1: {k3_total_f1/N}\")\n",
    "print(f\"MAP: {k3_total_map/N}\")\n",
    "print(f\"NDCG: {k3_total_ndcg/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e118ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
