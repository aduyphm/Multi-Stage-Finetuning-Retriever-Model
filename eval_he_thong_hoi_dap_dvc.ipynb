{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8590cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, InputExample\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from torch.utils.data import DataLoader\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6018f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu test\n",
    "with open(\"dvc_test.json\", 'r') as file:\n",
    "    test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73eaaa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu các văn bản\n",
    "vbpl = pd.read_csv('sent_truncated_vbpl_update.csv', encoding='utf-16')\n",
    "vbpl = vbpl.dropna().reset_index(drop=True)\n",
    "passages = vbpl['truncated_text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6eff8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segment(sent):\n",
    "    sent = tokenize(sent.encode('utf-8').decode('utf-8'))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d42e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f2(precision, recall):        \n",
    "    return (5 * precision * recall) / (4 * precision + recall + 1e-20)\n",
    "def calculate_f1(precision, recall):        \n",
    "    return (precision * recall) / (precision + recall + 1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9b361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/simcse-model-phobert-base-1x.pkl\", \"r\") as file:\n",
    "    bi_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33aea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/ce-model-05.pkl\", \"r\") as file:\n",
    "    cross_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24051b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"model/bm25_plus\", \"rb\") as bm_file:\n",
    "    bm25 = pickle.load(bm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4b3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.read_index(\"model/case_1x.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405e9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(hits, k, scores):\n",
    "    true_positive_set = set()\n",
    "    false_positive_set = set()\n",
    "    num_hits = 0\n",
    "    average_precision = 0\n",
    "    actual_relevance = []\n",
    "      \n",
    "    for j, idx_pred in enumerate(hits[:k].index, 1):\n",
    "        key = (hits.at[idx_pred, \"so_hieu\"], hits.at[idx_pred, \"dieu\"])\n",
    "        if key in dict_relevant:\n",
    "            actual_relevance.append(1)\n",
    "            true_positive_set.add(key)\n",
    "            num_hits += 1\n",
    "            average_precision += num_hits/j\n",
    "        else:\n",
    "            actual_relevance.append(0)\n",
    "            false_positive_set.add(key)\n",
    "\n",
    "    true_positive = len(true_positive_set)            \n",
    "    false_positive = len(false_positive_set)\n",
    "    \n",
    "    if num_hits != 0: \n",
    "        average_precision = average_precision/num_hits\n",
    "    \n",
    "    ndcg = ndcg_score([actual_relevance], [scores[:k]], k=k)\n",
    "    \n",
    "    precision = true_positive/(true_positive + false_positive + 1e-20)\n",
    "    recall = true_positive/actual_positive\n",
    "    f1 = calculate_f1(precision, recall)\n",
    "    f2 = calculate_f2(precision, recall)\n",
    "    return precision, recall, f1, f2, average_precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79233cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [2:16:28, 14.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá với dữ liệu\n",
    "top_k = 500\n",
    "top_k1 = 20\n",
    "top_k2 = 5\n",
    "thresh = 0\n",
    "k1_total_f1 = 0\n",
    "k1_total_f2 = 0\n",
    "k1_total_precision = 0\n",
    "k1_total_recall = 0\n",
    "k1_total_map = 0\n",
    "k1_total_ndcg = 0\n",
    "k2_total_f1 = 0\n",
    "k2_total_f2 = 0\n",
    "k2_total_precision = 0\n",
    "k2_total_recall = 0\n",
    "k2_total_map = 0\n",
    "k2_total_ndcg = 0\n",
    "k3_total_f1 = 0\n",
    "k3_total_f2 = 0\n",
    "k3_total_precision = 0\n",
    "k3_total_recall = 0\n",
    "k3_total_map = 0\n",
    "k3_total_ndcg = 0\n",
    "\n",
    "\n",
    "for i, item in tqdm(enumerate(test)):\n",
    "    query = item[\"noi_dung_hoi\"]\n",
    "    query_bm25 = query[:-1] if query.endswith(\"?\") else query\n",
    "    relevant_articles = item[\"vb_lien_quan\"]\n",
    "    dict_relevant = {(article[\"so_hieu\"], article[\"dieu\"]) : article for article in relevant_articles}\n",
    "    actual_positive = len(relevant_articles)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Block 1: Retrieve top k using BM25+ ####\n",
    "    query_seg = word_segment(query_bm25)\n",
    "    query_tokens = query_seg.split()\n",
    "    doc_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    predictions = np.argpartition(doc_scores, len(doc_scores) - top_k)[-top_k:]\n",
    "    hits_bm25 = vbpl.iloc[predictions]\n",
    "    \n",
    "    hits3 = hits_bm25.copy()\n",
    "    hits3[\"bm25-score\"] = doc_scores[:top_k] / (np.linalg.norm(doc_scores[:top_k])+1e-20)\n",
    "    \n",
    "    #### Rerank using bi-encoder top k1 ####\n",
    "    embeddings = bi_encoder.encode(hits3['truncated_text'].to_list())\n",
    "    query_embeddings = bi_encoder.encode([query])\n",
    "    cos_scores = []\n",
    "    for embedding in embeddings:\n",
    "        cos_score = util.cos_sim(query_embeddings, embedding)\n",
    "        cos_scores.append(cos_score.numpy()[0,0])\n",
    "    hits3['cos-score'] = cos_scores\n",
    "#     hits3['ensem-score'] = (1/2*hits3['cos-score']**2 + 1/2*hits3['bm25-score']**2)**1/2\n",
    "    hits3['ensem-score'] = (hits3['cos-score'] * hits3['bm25-score'])**1/2\n",
    "    \n",
    "    hits3 = hits3.sort_values('ensem-score', ascending=False)\n",
    "    \n",
    "#     #### Rerank using cross-encoder ####  \n",
    "#     cross_inp = [[query, passages[idx]] for idx in hits3[:top_k1].index]\n",
    "#     cross_scores = cross_encoder.predict(cross_inp)\n",
    "#     hits4 = hits3[:top_k1].copy()\n",
    "#     hits4['cross-score'] = cross_scores\n",
    "\n",
    "    \n",
    "    \n",
    "    #### Block 2: Retrieve top k using SimCSE ####\n",
    "    query_embedding = bi_encoder.encode([query])\n",
    "    normalized_query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    scores, indices, embeddings = index.search_and_reconstruct(normalized_query_embedding, top_k)\n",
    "    hits_simcse = vbpl.iloc[indices[0]]\n",
    "    hits1 = hits_simcse.copy()\n",
    "    hits1['cos-score'] = scores[0]\n",
    "    \n",
    "    #### Rerank using BM25+ 2-grams top k1 ####\n",
    "    corpus = []\n",
    "    ngram_corpus2 = []\n",
    "    for p in hits_simcse['truncated_text']:\n",
    "        p = word_segment(p)\n",
    "        p_tokens = p.split()\n",
    "        corpus.append(p_tokens)\n",
    "        ngram_corpus2 = [[ngram for ngram in zip(*[tokens[i:] for i in range(2)])] for tokens in corpus]\n",
    "    \n",
    "    bm25_rerank = BM25Plus(ngram_corpus2)\n",
    "    \n",
    "    query_seg = word_segment(query)\n",
    "    query_tokens = query_seg.split()\n",
    "    query_tokens_2 = [ngram for ngram in zip(*[query_tokens[i:] for i in range(2)])]\n",
    "    bm25_scores = bm25_rerank.get_scores(query_tokens_2)\n",
    "    hits1['bm25-score'] = bm25_scores / (np.linalg.norm(bm25_scores)+1e-20)\n",
    "#     hits1['ensem-score'] = (1/2*hits1['bm25-score']**2 + 1/2*hits1['score']**2)**1/2\n",
    "    hits1['ensem-score'] = (hits1['bm25-score']*hits1['cos-score'])**1/2\n",
    "    hits1 = hits1.sort_values('ensem-score', ascending=False)\n",
    "    \n",
    "#     #### Rerank using cross-encoder ####    \n",
    "#     cross_inp = [[query, passages[idx]] for idx in hits1[:top_k1].index]\n",
    "#     cross_scores = cross_encoder.predict(cross_inp)\n",
    "#     hits2 = hits1[:top_k1].copy()\n",
    "#     hits2['cross-score'] = cross_scores\n",
    "    \n",
    "    \n",
    "    #### Block 3: Ensemble Stage ####\n",
    "    combine = pd.concat([hits1[:20], hits3[:20]])\n",
    "    combine = combine.sort_values('ensem-score', ascending=False)\n",
    "    hits_ensem = combine.drop_duplicates(subset=['so_hieu', 'dieu', 'truncated_text'], keep='first')\n",
    "    \n",
    "#     hits_ensem = hits_ensem.sort_values('ensem-score', ascending=False)\n",
    "    \n",
    "    precision, recall, f1, f2, average_precision, ndcg = evaluate(hits=hits_ensem, k=top_k1, scores=hits_ensem['ensem-score'].to_list())\n",
    "\n",
    "    k1_total_precision += precision\n",
    "    k1_total_recall += recall\n",
    "    k1_total_f1 += f1\n",
    "    k1_total_f2 += f2\n",
    "    k1_total_map += average_precision\n",
    "    k1_total_ndcg += ndcg\n",
    "    \n",
    "    precision, recall, f1, f2, average_precision, ndcg = evaluate(hits=hits_ensem, k=top_k2, scores=hits_ensem['ensem-score'].to_list())\n",
    "\n",
    "    k2_total_precision += precision\n",
    "    k2_total_recall += recall\n",
    "    k2_total_f1 += f1\n",
    "    k2_total_f2 += f2\n",
    "    k2_total_map += average_precision\n",
    "    k2_total_ndcg += ndcg\n",
    "    \n",
    "    precision, recall, f1, f2, average_precision, ndcg = evaluate(hits=hits_ensem, k=hits_ensem.shape[0], scores=hits_ensem['ensem-score'].to_list())\n",
    "\n",
    "    k3_total_precision += precision\n",
    "    k3_total_recall += recall\n",
    "    k3_total_f1 += f1\n",
    "    k3_total_f2 += f2\n",
    "    k3_total_map += average_precision\n",
    "    k3_total_ndcg += ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305b22ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 20\n",
      "Recall: 0.8634978671541742\n",
      "Precision: 0.06480890732104808\n",
      "F2: 0.24228212818558523\n",
      "F1: 0.05956218072503938\n",
      "MAP: 0.6286834254454082\n",
      "NDCG: 0.7053359014830364\n"
     ]
    }
   ],
   "source": [
    "N = len(test)\n",
    "print(f\"k = {top_k1}\")\n",
    "print(f\"Recall: {k1_total_recall/N}\")\n",
    "print(f\"Precision: {k1_total_precision/N}\")\n",
    "print(f\"F2: {k1_total_f2/N}\")\n",
    "print(f\"F1: {k1_total_f1/N}\")\n",
    "print(f\"MAP: {k1_total_map/N}\")\n",
    "print(f\"NDCG: {k1_total_ndcg/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d0dbc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "Recall: 0.7583790371724559\n",
      "Precision: 0.224314442413163\n",
      "F2: 0.49298741072049934\n",
      "F1: 0.16721946548272038\n",
      "MAP: 0.6456073532398938\n",
      "NDCG: 0.6854849443011977\n"
     ]
    }
   ],
   "source": [
    "N = len(test)\n",
    "print(f\"k = {top_k2}\")\n",
    "print(f\"Recall: {k2_total_recall/N}\")\n",
    "print(f\"Precision: {k2_total_precision/N}\")\n",
    "print(f\"F2: {k2_total_f2/N}\")\n",
    "print(f\"F1: {k2_total_f1/N}\")\n",
    "print(f\"MAP: {k2_total_map/N}\")\n",
    "print(f\"NDCG: {k2_total_ndcg/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "202d1626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 40\n",
      "Recall: 0.8895490554539914\n",
      "Precision: 0.04747139764060302\n",
      "F2: 0.19018464250455538\n",
      "F1: 0.04459440720959348\n",
      "MAP: 0.6280865369375142\n",
      "NDCG: 0.7109856557674793\n"
     ]
    }
   ],
   "source": [
    "N = len(test)\n",
    "print(f\"k = {2*top_k1}\")\n",
    "print(f\"Recall: {k3_total_recall/N}\")\n",
    "print(f\"Precision: {k3_total_precision/N}\")\n",
    "print(f\"F2: {k3_total_f2/N}\")\n",
    "print(f\"F1: {k3_total_f1/N}\")\n",
    "print(f\"MAP: {k3_total_map/N}\")\n",
    "print(f\"NDCG: {k3_total_ndcg/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb0ea58",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "nguyenvulebinh/vi-mrc-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/nguyenvulebinh/vi-mrc-base/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1194\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[0;32m   1196\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1540\u001b[0m     )\n\u001b[1;32m-> 1541\u001b[1;33m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    276\u001b[0m             )\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-64d72fed-2b2a31696aea3d9606b45ded;8c0155c3-eb10-44d0-b378-b0f49138c605)\n\nCannot access gated repo for url https://huggingface.co/nguyenvulebinh/vi-mrc-base/resolve/main/config.json.\nRepo model nguyenvulebinh/vi-mrc-base is gated. You must be authenticated to access it.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-826822a31c23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nguyenvulebinh/vi-mrc-base\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m nlp = pipeline('question-answering', model=model_checkpoint,\n\u001b[0m\u001b[0;32m      4\u001b[0m                    tokenizer=model_checkpoint)\n\u001b[0;32m      5\u001b[0m QA_input = {\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[0mhub_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_commit_hash\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m         \u001b[0mhub_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_commit_hash\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name_or_path\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trust_remote_code\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"auto_map\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"AutoConfig\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"auto_map\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;31m# Get config dict associated with the base config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_commit_hash\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[1;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[0;32m    673\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\APPLICATION\\Anaconda3\\envs\\course\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[1;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: nguyenvulebinh/vi-mrc-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_checkpoint = \"nguyenvulebinh/vi-mrc-base\"\n",
    "nlp = pipeline('question-answering', model=model_checkpoint,\n",
    "                   tokenizer=model_checkpoint)\n",
    "QA_input = {\n",
    "    'question': 'Hồ sơ của thủ tục cấp Giấy phép tổ chức thi người đẹp, người mẫu trong phạm vi địa phương gồm những gì? Thời gian thực hiện trong bao lâu?',\n",
    "    'context': 'Điều 21 79/2012/NĐ-CP quy định về biểu diễn nghệ thuật, trình diễn thời trang; thi người đẹp và người mẫu; lưu hành, kinh doanh bản ghi âm, ghi hình ca múa nhạc, sân khấu Thủ tục cấp giấy phép tổ chức thi người đẹp, người mẫu 1. Tổ chức đề nghị cấp giấy phép tổ chức thi người đẹp, người mẫu gửi 01 bộ hồ sơ trực tiếp hoặc qua đường bưu điện, đến Cục Nghệ thuật biểu diễn hoặc Sở Văn hóa, Thể thao và Du lịch. Hồ sơ gồm: a) 01 đơn đề nghị cấp giấy phép tổ chức cuộc thi (Mẫu số 04); b) 01 đề án tổ chức cuộc thi, trong đó nêu rõ: Thể lệ cuộc thi, quy chế hoạt động của Ban tổ chức, Ban giám khảo; c) 01 văn bản đồng ý của Ủy ban nhân dân cấp tỉnh, nơi dự định tổ chức; d) 01 bản sao chứng thực hợp đồng hoặc văn bản thỏa thuận giữa tổ chức Việt Nam với tổ chức nước ngoài (bản dịch tiếng Việt có chứng nhận của công ty dịch thuật, đối với cuộc thi người đẹp, người mẫu quốc tế tổ chức tại Việt Nam). 2. Tổ chức nước ngoài tổ chức cuộc thi người đẹp, người mẫu quốc tế tại Việt Nam phải phối hợp với tổ chức Việt Nam có chức năng hoạt động văn hóa nghệ thuật. Tổ chức Việt Nam nộp hồ sơ theo quy định tại Khoản 1 Điều này. 3. Thời hạn cấp phép: Trong thời hạn 15 ngày làm việc (đối với cuộc thi người đẹp, người mẫu trong nước) và 30 ngày làm việc (đối với cuộc thi người đẹp, người mẫu quốc tế tổ chức tại Việt Nam), kể từ ngày nhận đủ hồ sơ hợp lệ, cơ quan nhà nước có thẩm quyền cấp giấy phép. Trường hợp không cấp giấy phép phải trả lời bằng văn bản và nêu rõ lý do.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "print('pipeline: {}'.format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e118ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
